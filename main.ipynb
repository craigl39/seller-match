{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_sample = pd.read_excel('./dataset/MATCHED SAMPLE.xlsx')\n",
    "matched_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_sample.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seller_master = pd.read_csv('./dataset/Seller Master.csv', on_bad_lines='skip', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seller_master.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seller_master.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marketplace_listings = pd.read_excel('./dataset/Marketplace Listings G2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marketplace_listings.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marketplace_listings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat ListingDescription1, ListingDescription2, and ListingDescription3\n",
    "marketplace_listings['Description'] = marketplace_listings['ListingDescription1'] + marketplace_listings['ListingDescription2'] + marketplace_listings['ListingDescription3']\n",
    "\n",
    "marketplace_listings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seller_master_df = seller_master[['SellerId', 'SellerName', 'SellerAlias', 'SellerUrl', 'SellerDescription']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seller_master_df.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seller_master_df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seller_master_df.dropna(subset=['SellerId'], inplace=True)\n",
    "seller_master_df['SellerName'] = seller_master_df['SellerName'].astype('str')\n",
    "seller_master_df['SellerAlias'] = seller_master_df['SellerName'].astype('str')\n",
    "seller_master_df['SellerUrl'] = seller_master_df['SellerUrl'].astype('str')\n",
    "seller_master_df['SellerDescription'] = seller_master_df['SellerDescription'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seller_master_df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seller_master_df.drop_duplicates(subset=['SellerId'], inplace=True)\n",
    "seller_master_df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seller_master_df.head(651)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_records = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_url(url):\n",
    "    try:\n",
    "        return url.split('//')[1].split('/')[0]\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "def preprocess_name(name):\n",
    "    cleaned_name = name\n",
    "    cleaned_name = cleaned_name.replace(' ', '-')\n",
    "    cleaned_name = cleaned_name.replace(\"%\", \"amp\")\n",
    "    cleaned_name = cleaned_name.replace('.', 'dot')\n",
    "    cleaned_name = cleaned_name.replace('(', 'open-bracket-')\n",
    "    cleaned_name = cleaned_name.replace(')', '-close-bracket')\n",
    "    \n",
    "    return cleaned_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sentence list for the first 10000 records\n",
    "seller_name_list = seller_master_df['SellerName'].head(train_records).tolist()\n",
    "seller_alias_list = seller_master_df['SellerAlias'].head(train_records).tolist()\n",
    "seller_url_list = seller_master_df['SellerUrl'].head(train_records).tolist()\n",
    "seller_description_list = seller_master_df['SellerDescription'].head(train_records).tolist()\n",
    "\n",
    "seller_sentence_list = pd.concat([\n",
    "    pd.Series([preprocess_name(name) for name in seller_name_list]),\n",
    "    pd.Series([preprocess_name(name) for name in seller_alias_list]),\n",
    "    pd.Series([preprocess_url(url) for url in seller_url_list]),\n",
    "    # pd.Series(seller_description_list)\n",
    "]).drop_duplicates().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seller_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "data = []\n",
    "\n",
    "# iterate through each sentence in the file\n",
    "for sentence in seller_sentence_list:\n",
    "      temp = []\n",
    "      for word in word_tokenize(sentence):\n",
    "            temp.append(word)\n",
    "      \n",
    "      data.append(temp)\n",
    "\n",
    "# Create CBOW model\n",
    "model1 = Word2Vec(data, min_count=1, vector_size=300, window=5)\n",
    "\n",
    "model1.wv.save_word2vec_format(\"./model/word2vec.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gzip ./model/word2vec.txt -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy init vectors en ./model/word2vec.txt.gz output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"./output\")\n",
    "\n",
    "# Preprocess textual features and build index\n",
    "index_to_id_map = {}\n",
    "index_count = 0\n",
    "\n",
    "# Split data into batches\n",
    "batch_size = 1000\n",
    "num_batches = -(-len(seller_master_df) // batch_size)  # Calculate number of batches rounding up\n",
    "batches = [seller_master_df[i*batch_size:(i+1)*batch_size] for i in range(10)]\n",
    "\n",
    "ann_idx = 0\n",
    "\n",
    "# Batch processing function to generate embeddings\n",
    "def process_batch(batch):\n",
    "    global ann_idx\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    for i, row in batch.iterrows():\n",
    "        seller_id = row['SellerId']\n",
    "        \n",
    "        name = preprocess_name(row['SellerName'])\n",
    "        alias = preprocess_name(row['SellerAlias'])\n",
    "        url = preprocess_url(row['SellerUrl'])\n",
    "        \n",
    "        texts = [name, alias] if name != alias else [name]\n",
    "        if url != \"\":\n",
    "            texts.append(url)\n",
    "        \n",
    "        for text in texts:\n",
    "            doc = nlp(text)\n",
    "            if np.all(doc.vector == 0):\n",
    "                print(f\"Zero vector found for text: {text}\")\n",
    "                continue\n",
    "    \n",
    "            embeddings.append((ann_idx, doc.vector))\n",
    "            index_to_id_map[ann_idx] = seller_id\n",
    "            ann_idx += 1\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "emb_batch = [process_batch(batch) for batch in batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n",
    "\n",
    "ann_index = AnnoyIndex(300, 'angular')  # Assuming we are using 300-dimensional embeddings from spaCy\n",
    "ann_index.verbose(True)\n",
    "\n",
    "for embeddings in emb_batch:\n",
    "    for index, emb in embeddings:\n",
    "        ann_index.add_item(index, emb)\n",
    "\n",
    "ann_index.build(10)\n",
    "ann_index.save('./model/seller_master.ann')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save index to seller id map\n",
    "pd.DataFrame(index_to_id_map.items(), columns=['index', 'SellerId']).to_csv('./model/seller_id_index_map.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_index = AnnoyIndex(300, 'angular')\n",
    "ann_index.load('./model/seller_master.ann')\n",
    "\n",
    "index_to_id_map = pd.read_csv('./model/seller_id_index_map.csv').set_index('index').to_dict()['SellerId']\n",
    "\n",
    "# Function to find the most similar sellerId for a given query\n",
    "def find_similar_seller(query_name, query_url, query_desc):\n",
    "    query_name_emb = nlp(preprocess_name(query_name)).vector\n",
    "    query_url_emb = nlp(preprocess_url(query_url)).vector\n",
    "    # query_desc_emb = nlp(query_desc).vector\n",
    "    \n",
    "    # find the best score\n",
    "    best_score = -1\n",
    "    best_index = -1\n",
    "    for emb in [query_name_emb, query_url_emb]:\n",
    "        index = ann_index.get_nns_by_vector(emb, 1)[0]\n",
    "        score = np.dot(emb, ann_index.get_item_vector(index))\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_index = index\n",
    "    \n",
    "    return index_to_id_map[best_index]\n",
    "print(find_similar_seller('0X0CAT', 'https://www.0x0.cat', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seller_master_df.head(651)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
